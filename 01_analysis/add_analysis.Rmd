---
title: "Supplemental Analyses"
author: "Jennah Gosciak"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: false
---

```{r setup, set.seed(1690)}
```

```{r, include = F}
#options(warn=-1)
suppressPackageStartupMessages(library(tibble))
library(tidyverse)
library(knitr)
library(rgl)
library(rstan)
options(mc.cores = parallel::detectCores())
library(readr)
library(dplyr)
library(ggplot2)
library(purrr)
library(rstanarm)
library(loo)
library(bayesplot)
library(haven)

set.seed(1690)
```


```{r, include = F}
## load data
df <- read_dta("../00_data/sample1.dta")
df_samp <- df %>% 
  sample_n(3000)

df_samp <- df_samp %>% 
  mutate(across(c("age", "age_fbirth"), ~ . - mean(., na.rm = T))) %>% 
  mutate(l_incwage = if_else(incwage <= 0, log(1), log(incwage)),
         l_wkswork1 = if_else(wkswork1 <= 0, log(1), log(wkswork1)),
         incwage_mod = if_else(incwage <= 0, 1, incwage))
```

# Analysis

**Note:**
This PDF uses the same data as [final_project.pdf](https://github.com/jennahgosciak/APSTA2123_final/blob/main/01_analysis/final_project.pdf). It implements three models:

1. Gamma GLM with a log link
2. hierarchical linear model where intercepts vary by state
3. hierarchical IV model where intercepts vary by state


## Gamma GLM

* I explored using the gamma distribution with a log link function since the income data is skewed and positive.
* The implementation should be very similar to the results generated by `rstanarm` with `stan_glm` and `family = Gamma(link = "log")`.


```{r}
# display code
writeLines(readLines("linear_gamma.stan"))
```


```{r, cache = TRUE}
# use normal priors
m <- c(10, rep(-0.1, 8))
s<- rep(1, 9)

stan_data <- list(N = nrow(df_samp), K = 8, y = df_samp$incwage_mod, 
                                        X = df_samp[, c("cnum_mt2", "age", 
                                                        "age_fbirth", "f_boy", 
                                                   "s_boy", "r_black", 
                                                   "hisp", "r_oth")],
                                        prior_only = TRUE, m = m, 
                                        scale = s, r = 1)

pre_gamma <- stan("linear_gamma.stan", data = stan_data, seed = 12345)
```
```{r}
# print output
print(pre_gamma, pars = c("alpha", "beta", "shape"))
```

```{r}
hist(rstan::extract(pre_gamma, par = "yrep")$yrep,
     main = "Prior predictive distribution",
     xlab = "yrep")
```

The prior predictive distribution generates large, extreme values of y. At the same time, most observations are clustered toward 0.

```{r, cache = TRUE}
# set prior to false, run the full dist
stan_data$prior_only <- FALSE

post_gamma <- stan("linear_gamma.stan", data = stan_data,
                   seed = 12345)
```

```{r}
# print output
print(post_gamma, pars = c("alpha", "beta", "shape"))
```

The posterior distribution from the Gamma GLM aligns with results from the other linear models, those that use the normal distribution for the likelihood. The coefficient on `cnum_mt2` is likely negative. On average it is -0.58 and the 95% credible interval is between -0.77 and -0.4. Since this model uses a log link, the estimated coefficient is in log units. A coefficient of -0.58 indicates a decrease of approximately 44%.

```{r}
pairs(post_gamma, pars = c("alpha", "beta", "shape"))
```

The marginal distributions for the parameters all seem approximately normal, even for shape. 

```{r}
pp_check(as.numeric(stan_data$y),
  rstan::extract(post_gamma, par = "yrep")$yrep[sample(1:length(stan_data$y), size = 150), ],
  ppc_dens_overlay
)
```

The posterior predictive distribution does appear to closely follow the observed values of y, which are less smooth. However, it's difficult to tell in the tails if the observed values and the predicted values diverge. The predicted values of y may be more extreme than the observed values of y.

```{r}
# comparison of predictions
yrep <- rstan::extract(post_gamma, "yrep")[[1]]
low  <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 1 / 3)
high <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 2 / 3)
```

```{r}
y <- stan_data$y

c(too_low = mean(y < low), 
  just_right = mean(y > low & y < high),
  too_high = mean(y > high))
```

Ideally, this would be $\frac{1}{3}$ for all three tertiles. The model is predicting too many extreme values (bottom and top third percentiles). Only 9% of observed y-values fall in only the middle third percentile.

```{r, cache = TRUE}
loo_gamma2 <- loo(post_gamma)
loo_gamma2
```

All the Pareto k values are less than 0.5, which indicates that importance sampling can generate a good estimate. Additionally, `p_loo` $\approx$ `p`, which provides no evidence of any model misspecification.

## Hierarchical linear model

* Allowing intercepts to shift based on state
* Replicates the linear model from the main analysis

```{r, cache = TRUE}
# set generic priors
m <- rep(0, 9)
s<- rep(1, 9)

# set states as ordered factor
states <- as.integer(as.factor(df_samp$statefip))


stan_data <- list(N = nrow(df_samp), K = 8, J = 51,
                                        states = states,
                                        y = df_samp$l_incwage, 
                                        # use fewer predictors
                                        X = df_samp[, c("cnum_mt2", "age", 
                                                        "age_fbirth", "f_boy",
                                                        "s_boy",
                                                        "r_black", "hisp", 
                                                        "r_oth")],
                                        prior_only = FALSE, m = m, 
                                        scale = s, r = 1)

post_mlm <- stan("linear_mlm.stan", data = stan_data, seed = 1234)
```
```{r}
print(post_mlm, pars = c("alpha", "beta"))
```

When the alpha values vary, the coefficient on `cnum_mt2` is inconclusive. The 95% credible interval is from -0.56 to 0.11 and the mean is -0.23. Considering that the outcome is in log units, this indicates a broad range of effects and a high level of uncertainty about the impact of any one covariate.


```{r, cache = TRUE}
loo_mlm <- loo(post_mlm)
loo_mlm
```

All Pareto k estimates are less than 0.5, which indicates that importance sampling is able to estimate the `elpd_loo` accurately and there are few observed values that have an outsized influence. The `elpd_loo` will be useful for comparing to other models.

```{r}
## visualize density
pp_check(as.numeric(stan_data$y),
  rstan::extract(post_mlm, par = "yrep")$yrep[sample(1:length(stan_data$y), size = 150), ],
  ppc_dens_overlay
)

# comparison of predictions
yrep <- rstan::extract(post_mlm, "yrep")[[1]]
low  <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 1 / 3)
high <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 2 / 3)

y <- stan_data$y

c(too_low = mean(y < low), 
  just_right = mean(y > low & y < high),
  too_high = mean(y > high))
```

Since the observed values of y appear to be bimodal, the posterior predictive distribution, which is approximately normal, has trouble predicting values at both ends. 

The model is predicting too many extreme values (in the bottom and top tertile). In terms of prediction, it is slightly better than the previous model with the gamma distribution and a log link. 10% of observed y values fall within the middle tertile.

## Hierarchical IV model

* Allowing intercepts to shift based on state
* Replicates the IV model from the main analysis

```{r}
# set generic priors
m <- rep(0, 9)
s<- rep(1, 9)

# set states as ordered factor
df_samp$states <- as.integer(as.factor(df_samp$statefip))

# subset the data
df_child <- df_samp %>% 
  filter(cnum_mt2 == 1)

df_nochild <- df_samp %>% 
  filter(cnum_mt2 == 0)
```

```{r, cache = TRUE}
# reset covariates, use only 4 covariates
cov <- c("age", "age_fbirth", "f_boy", "s_boy", "r_black", "hisp", "r_oth")

# set stan data
stan_data_iv <- list(N = nrow(df_samp),
                  N_child = nrow(df_child),
                  N_nochild = nrow(df_nochild),
                  K = 7,
                  J = 51,
                  states_child = df_child$states,
                  states_nochild = df_nochild$states,
                  X_child_s = df_child[, c("samesex", cov)],
                  X_nochild_s = df_nochild[, c("samesex", cov)],
                  X_child = df_child[, c(cov)],
                  X_nochild = df_nochild[, c(cov)],
                  y_child = df_child$l_incwage,
                  y_nochild = df_nochild$l_incwage,
                  prior_only = FALSE,
                  m = rep(-0.1, 5), 
                  scale = rep(0.3, 5))

post_iv_mlm <- stan("iv_bin_mlm.stan",
                 data = stan_data_iv, seed = 1234)
```

```{r}
print(post_iv_mlm, pars = c("beta1", "beta2", "alpha0", "alpha1"))
```

Based on the value of `beta2`, the output indicates that the impact of `cnum_mt2` is likely positive when including an instrument to isolate the causal effect and allowing intercepts to vary by state. The average estimated value of `beta2` is 4.13 and the 95% credible interval is from 3.83 to 4.62. Similar to the non-hierarchical models, there may be some limitations to the interpretation of a true effect from this model:

1. it only uses 3,000 observations from the full sample
2. it does not account for the fact that income is conditional on labor force participation
3. the model does not have good predictive performance compared to the simpler model when evaluated using cross-validation


```{r}
loo_iv_mlm <- loo(post_iv_mlm)
loo_iv_mlm

loo_compare(loo_iv_mlm, loo_mlm)
```

The simpler linear model appears to be the better model in terms of prediction accuracy based on cross-validation. The `elpd_diff` value is much larger than the SE.

```{r}
# visualize density
pp_check(c(as.numeric(stan_data_iv$y_child),
           as.numeric(stan_data_iv$y_nochild)),
  rstan::extract(post_iv_mlm, par = "yrep")$yrep[sample(1:nrow(df_samp),
                                                    size = 150), ],
  ppc_dens_overlay
)

# comparison of predictions
yrep <- rstan::extract(post_iv_mlm, "yrep")[[1]]
low  <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 1 / 3)
high <- apply(yrep, MARGIN = 2, FUN = quantile, probs = 2 / 3)

y <- c(as.numeric(stan_data_iv$y_child),
           as.numeric(stan_data_iv$y_nochild))

c(too_low = mean(y < low), 
  just_right = mean(y > low & y < high),
  too_high = mean(y > high))
```

As with the other linear model, the approximately normal posterior predictive distribution does not fully align with the bimodal distribution of observed y values. However, in terms of prediction accuracy, the hierarchical IV model makes better predictions than other models, including the non-hierarchical IV model and the linear models. Almost exactly $\frac{1}{3}$ of observed y values fall within the predicted bottom tertile. The model is still making too many extreme predictions for the top tertile since around 50% of observed y values fall within the predicted top tertile.

## Conclusion

With these supplemental analyses, I hoped to explore whether any of modifications to the models identify starkly different trends or have improved prediction accuracy. Interestingly, the last hierarchical IV model had the best prediction accuracy and it also estimated a large, positive impact of `cnum_mt2`. However, with cross validation, the simpler linear model did perform better. This makes sense since a more complex model is likely prone to overfitting. Prediction and causation are also different aims and so predictive accuracy does not necessarily reveal anything about the strength of the causal claims indicated by the model.

While the Gamma distribution seemed a good fit for this type of data since income cannot be negative, in terms of predictive accuracy it didn't generate a significant improvement. Visually, however, the predicted y values drawn from the gamma distribution do align better with the data compared to analyses that the other models, even those that use a log-transformed outcome.

As in the main analysis, the IV results contradict the frequentist study and indicate a large, positive impact of `cnum_mt2` on income. The same limitations apply here; these exploratory analyses simply provide a starting point for future study.




